apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: DockerCluster
metadata:
  name: ${CLUSTER_NAME}
  annotations:
    "helm.sh/resource-policy": keep
spec:
  failureDomains:
    fd1:
      controlPlane: true
    fd2:
      controlPlane: true
    fd3:
      controlPlane: true
    fd4:
      controlPlane: false
    fd5:
      controlPlane: false
    fd6:
      controlPlane: false
    fd7:
      controlPlane: false
    fd8:
      controlPlane: false
---
apiVersion: cluster.x-k8s.io/v1beta1
kind: Cluster
metadata:
  labels:
    cni: ${CLUSTER_NAME}-crs-0
  name: ${CLUSTER_NAME}
spec:
  clusterNetwork:
    pods:
      cidrBlocks:
      - 10.1.0.0/16
    serviceDomain: cluster.local
    services:
      cidrBlocks:
      - 10.10.0.0/16
  controlPlaneRef:
    apiVersion: controlplane.cluster.x-k8s.io/v1beta1
    kind: KubeadmControlPlane
    name: ${CLUSTER_NAME}-control-plane
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: DockerCluster
    name: ${CLUSTER_NAME}
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: DockerMachineTemplate
metadata:
  name: ${CLUSTER_NAME}-control-plane
spec:
  template:
    spec:
      extraMounts:
      - containerPath: /var/run/docker.sock
        hostPath: /var/run/docker.sock
---
apiVersion: controlplane.cluster.x-k8s.io/v1beta1
kind: KubeadmControlPlane
metadata:
  name: ${CLUSTER_NAME}-control-plane
  annotations:
    "helm.sh/resource-policy": keep
spec:
  kubeadmConfigSpec:
    clusterConfiguration:
      apiServer:
        certSANs:
        - localhost
        - 127.0.0.1
        - 0.0.0.0
        - host.docker.internal
      controllerManager:
        extraArgs:
          enable-hostpath-provisioner: "true"
    initConfiguration:
      nodeRegistration:
        criSocket: unix:///var/run/containerd/containerd.sock
        kubeletExtraArgs:
          eviction-hard: nodefs.available<0%,nodefs.inodesFree<0%,imagefs.available<0%
    joinConfiguration:
      nodeRegistration:
        criSocket: unix:///var/run/containerd/containerd.sock
        kubeletExtraArgs:
          eviction-hard: nodefs.available<0%,nodefs.inodesFree<0%,imagefs.available<0%
  machineTemplate:
    infrastructureRef:
      apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
      kind: DockerMachineTemplate
      name: ${CLUSTER_NAME}-control-plane
  replicas: ${CONTROL_PLANE_MACHINE_COUNT}
  version: ${KUBERNETES_VERSION}
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: DockerMachineTemplate
metadata:
  name: ${CLUSTER_NAME}-md-0
spec:
  template:
    spec:
      extraMounts:
      - containerPath: /var/run/docker.sock
        hostPath: /var/run/docker.sock
---
apiVersion: bootstrap.cluster.x-k8s.io/v1beta1
kind: KubeadmConfigTemplate
metadata:
  name: ${CLUSTER_NAME}-md-0
  annotations:
    "helm.sh/resource-policy": keep
spec:
  template:
    spec:
      joinConfiguration:
        nodeRegistration:
          criSocket: unix:///var/run/containerd/containerd.sock
          kubeletExtraArgs:
            eviction-hard: nodefs.available<0%,nodefs.inodesFree<0%,imagefs.available<0%
---
apiVersion: cluster.x-k8s.io/v1beta1
kind: MachineDeployment
metadata:
  name: ${CLUSTER_NAME}-md-0
  annotations:
    "helm.sh/resource-policy": keep
spec:
  clusterName: ${CLUSTER_NAME}
  replicas: ${WORKER_MACHINE_COUNT}
  selector:
    matchLabels: null
  template:
    spec:
      bootstrap:
        configRef:
          apiVersion: bootstrap.cluster.x-k8s.io/v1beta1
          kind: KubeadmConfigTemplate
          name: ${CLUSTER_NAME}-md-0
      clusterName: ${CLUSTER_NAME}
      failureDomain: fd4
      infrastructureRef:
        apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
        kind: DockerMachineTemplate
        name: ${CLUSTER_NAME}-md-0
      version: ${KUBERNETES_VERSION}
---
apiVersion: addons.cluster.x-k8s.io/v1beta1
kind: ClusterResourceSet
metadata:
  name: ${CLUSTER_NAME}-crs-0
spec:
  clusterSelector:
    matchLabels:
      cni: ${CLUSTER_NAME}-crs-0
  resources:
  - kind: ConfigMap
    name: cni-${CLUSTER_NAME}-crs-0
  strategy: ApplyOnce
---
apiVersion: v1
data:
  kindnet.yaml: |
    # kindnetd networking manifest
    ---
    kind: ClusterRole
    apiVersion: rbac.authorization.k8s.io/v1
    metadata:
      name: kindnet
    rules:
      - apiGroups:
          - ""
        resources:
          - nodes
        verbs:
          - list
          - watch
          - patch
      - apiGroups:
          - ""
        resources:
          - configmaps
        verbs:
          - get
    ---
    kind: ClusterRoleBinding
    apiVersion: rbac.authorization.k8s.io/v1
    metadata:
      name: kindnet
    roleRef:
      apiGroup: rbac.authorization.k8s.io
      kind: ClusterRole
      name: kindnet
    subjects:
      - kind: ServiceAccount
        name: kindnet
        namespace: kube-system
    ---
    apiVersion: v1
    kind: ServiceAccount
    metadata:
      name: kindnet
      namespace: kube-system
    ---
    apiVersion: apps/v1
    kind: DaemonSet
    metadata:
      name: kindnet
      namespace: kube-system
      labels:
        tier: node
        app: kindnet
        k8s-app: kindnet
    spec:
      selector:
        matchLabels:
          app: kindnet
      template:
        metadata:
          labels:
            tier: node
            app: kindnet
            k8s-app: kindnet
        spec:
          hostNetwork: true
          tolerations:
            - operator: Exists
              effect: NoSchedule
          serviceAccountName: kindnet
          containers:
            - name: kindnet-cni
              image: kindest/kindnetd:v20230511-dc714da8
              env:
                - name: HOST_IP
                  valueFrom:
                    fieldRef:
                      fieldPath: status.hostIP
                - name: POD_IP
                  valueFrom:
                    fieldRef:
                      fieldPath: status.podIP
                - name: POD_SUBNET
                  value: '10.1.0.0/16'
              volumeMounts:
                - name: cni-cfg
                  mountPath: /etc/cni/net.d
                - name: xtables-lock
                  mountPath: /run/xtables.lock
                  readOnly: false
                - name: lib-modules
                  mountPath: /lib/modules
                  readOnly: true
              resources:
                requests:
                  cpu: "100m"
                  memory: "50Mi"
                limits:
                  cpu: "100m"
                  memory: "50Mi"
              securityContext:
                privileged: false
                capabilities:
                  add: ["NET_RAW", "NET_ADMIN"]
          volumes:
            - name: cni-bin
              hostPath:
                path: /opt/cni/bin
                type: DirectoryOrCreate
            - name: cni-cfg
              hostPath:
                path: /etc/cni/net.d
                type: DirectoryOrCreate
            - name: xtables-lock
              hostPath:
                path: /run/xtables.lock
                type: FileOrCreate
            - name: lib-modules
              hostPath:
                path: /lib/modules
kind: ConfigMap
metadata:
  name: cni-${CLUSTER_NAME}-crs-0